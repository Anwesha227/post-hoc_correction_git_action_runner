name: Nebius Qwen Inference (ZIP, 6 shards, 12-file smoke + merge)

on:
  workflow_dispatch:
    inputs:
      drive_zip_file_id:
        description: "Google Drive FILE ID of semi-aves.zip (e.g., 16a9dvGMQwypqfAQ0vRtfMQzb7Aa7L2el)"
        required: true
      api_model:
        description: "Nebius model name"
        required: false
        default: "Qwen/Qwen2.5-VL-72B-Instruct"
      api_base:
        description: "Nebius OpenAI-compatible base URL"
        required: false
        default: "https://api.studio.nebius.com/v1/"
      parallel:
        description: "How many shards to run in parallel (1–6)"
        required: false
        default: "3"
      subset_count:
        description: "Run only first N lines from data/semi-aves/test.txt (0 = full). For smoke test use 12."
        required: false
        default: "12"

jobs:
  run:
    name: "Shard ${{ matrix.shard }} of ${{ matrix.total }}"
    runs-on: ubuntu-latest
    timeout-minutes: 350
    strategy:
      fail-fast: false
      max-parallel: ${{ fromJSON(github.event.inputs.parallel) }}
      matrix:
        shard: [0,1,2,3,4,5]   # 6 shards
        total: [6]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python3 -m pip install -U pip
          python3 -m pip install -r requirements.txt || true
          python3 -m pip install --upgrade "openai>=1.30.0" python-dotenv httpx pandas numpy pillow tqdm requests gdown
          # Avoid NumPy 2.x ↔ old pyarrow ABI mismatch
          python3 -m pip uninstall -y pyarrow || true

      - name: Install unzip
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip

      - name: Download dataset ZIP and unzip
        run: |
          set -euxo pipefail
          mkdir -p /tmp/dataset
          gdown "https://drive.google.com/uc?id=${{ github.event.inputs.drive_zip_file_id }}" -O /tmp/dataset/semi-aves.zip
          unzip -q /tmp/dataset/semi-aves.zip -d /tmp/dataset
          echo "After unzip:"
          find /tmp/dataset -maxdepth 2 -type d | sed 's/^/  /' | head -n 50

      - name: Sanity check dataset
        run: |
          set -euxo pipefail
          SEMI_AVES_DIR="$(find /tmp/dataset -maxdepth 2 -type d -name semi-aves | head -n1)"
          test -d "$SEMI_AVES_DIR/test"
          COUNT=$(find "$SEMI_AVES_DIR/test" -type f | wc -l)
          echo "Found $COUNT files under test/"
          [ "$COUNT" -ge 7000 ]

      - name: Expose dataset at ./datasets/semi-aves (as expected by wrapper)
        run: |
          mkdir -p datasets
          if [ -d /tmp/dataset/semi-aves ]; then
            ln -s /tmp/dataset/semi-aves datasets/semi-aves
          else
            CAND="$(find /tmp/dataset -maxdepth 2 -type d -name semi-aves | head -n1)"
            if [ -n "$CAND" ]; then
              ln -s "$CAND" datasets/semi-aves
            else
              echo "ERROR: Could not find 'semi-aves' folder in /tmp/dataset" >&2
              exit 1
            fi
          fi
          ls -la datasets

      - name: Provide .env for run_qwen.sh (NEBIUS_API_KEY)
        env:
          NEBIUS_API_KEY: ${{ secrets.NEBIUS_API_KEY }}
        run: |
          echo "NEBIUS_API_KEY=${NEBIUS_API_KEY}" > .env
          echo "NEBIUS_API_BASE=${{ github.event.inputs.api_base }}" >> .env
          echo ".env created at repo root"

      - name: Make scripts executable
        run: |
          chmod +x mllm-inference/semi-aves/scripts/run_nebius_72b.sh
          chmod +x mllm-inference/semi-aves/scripts/run_qwen.sh

      # -------- limit BEFORE sharding (smoke test) --------
      - name: Limit test.txt to first N lines (smoke)
        env:
          N: ${{ github.event.inputs.subset_count }}
        run: |
          set -euxo pipefail
          if [ -n "$N" ] && [ "$N" != "0" ]; then
            cp data/semi-aves/test.txt data/semi-aves/test_full.txt
            head -n "$N" data/semi-aves/test_full.txt > data/semi-aves/test.txt
            echo "Using first $N lines:"
            cat data/semi-aves/test.txt
          else
            echo "subset_count is 0 — running full list."
          fi

      # ---------------- SHARDING ----------------
      - name: Split test.txt into shards
        run: |
          # Create zero-padded parts: test_part00.txt … test_part05.txt
          split -n l/${{ matrix.total }} -d -a 2 \
            data/semi-aves/test.txt data/semi-aves/test_part --additional-suffix=.txt
          ls -la data/semi-aves/test_part*.txt
          wc -l data/semi-aves/test_part*.txt

      - name: Use this shard as test.txt (zero-padded index)
        run: |
          SH=$(printf "%02d" ${{ matrix.shard }})
          cp "data/semi-aves/test_part${SH}.txt" data/semi-aves/test.txt
          echo "Preview of this shard:"
          sed -n '1,20p' data/semi-aves/test.txt
          echo "Shard line count:"; wc -l data/semi-aves/test.txt

      - name: Clear proxy env (guard)
        run: |
          unset HTTP_PROXY HTTPS_PROXY http_proxy https_proxy ALL_PROXY all_proxy || true

      - name: Run your wrapper (which calls run_qwen.sh)
        run: |
          mllm-inference/semi-aves/scripts/run_nebius_72b.sh

      - name: Collect & upload outputs (tag with shard id)
        run: |
          set -euxo pipefail
          mkdir -p outputs
          shopt -s nullglob
          for f in mllm-inference/semi-aves/mllm_output/*.{csv,txt,log}; do
            base="$(basename "$f")"
            ext="${base##*.}"
            stem="${base%.*}"
            cp "$f" "outputs/${stem}_shard$(printf "%02d" ${{ matrix.shard }}).${ext}"
          done
          find outputs -maxdepth 1 -type f -print || true

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: qwen-results-shard-${{ matrix.shard }}
          path: outputs/
          if-no-files-found: warn

  merge:
    name: Merge shard outputs
    needs: run
    runs-on: ubuntu-latest
    steps:
      - name: Download shard artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: qwen-results-shard-*
          merge-multiple: true
          path: merged_inputs

      - name: Merge CSVs (single header)
        run: |
          set -euxo pipefail
          shopt -s nullglob
          mapfile -t FILES < <(find merged_inputs -maxdepth 2 -type f -name '*.csv' | sort)
          if [ "${#FILES[@]}" -eq 0 ]; then
            echo "No CSVs found to merge" >&2
            exit 1
          fi
          # write header from first file
          head -n 1 "${FILES[0]}" > merged.csv
          # append all bodies (skip headers)
          for f in "${FILES[@]}"; do
            tail -n +2 "$f" >> merged.csv || true
          done
          wc -l merged.csv
          ls -lh merged.csv

      - name: Merge error logs (optional)
        run: |
          set -euxo pipefail
          shopt -s nullglob
          ERRFILES=( $(find merged_inputs -type f -name '*errors*.txt' | sort) )
          if [ "${#ERRFILES[@]}" -gt 0 ]; then
            cat "${ERRFILES[@]}" > merged_errors.txt
            ls -lh merged_errors.txt
          else
            echo "No error logs to merge" > merged_errors.txt
          fi

      - name: Upload merged artifacts
        uses: actions/upload-artifact@v4
        with:
          name: qwen-results-merged
          path: |
            merged.csv
            merged_errors.txt
