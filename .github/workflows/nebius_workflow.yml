name: Nebius Qwen Inference (Drive ZIP + scripts, 6 shards)

on:
  workflow_dispatch:
    inputs:
      drive_zip_file_id:
        description: "Google Drive FILE ID of semi-aves.zip (e.g., 16a9dvGMQwypqfAQ0vRtfMQzb7Aa7L2el)"
        required: true
      api_model:
        description: "Nebius model name"
        required: false
        default: "Qwen/Qwen2.5-VL-72B-Instruct"
      api_base:
        description: "Nebius OpenAI-compatible base URL"
        required: false
        default: "https://api.studio.nebius.com/v1/"
      parallel:
        description: "How many shards to run in parallel (1–6)"
        required: false
        default: "3"

jobs:
  run:
    name: "Shard ${{ matrix.shard }} of ${{ matrix.total }}"
    runs-on: ubuntu-latest
    timeout-minutes: 350
    strategy:
      fail-fast: false
      max-parallel: ${{ fromJSON(github.event.inputs.parallel) }}
      matrix:
        shard: [0,1,2,3,4,5]   # 6 shards
        total: [6]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python3 -m pip install -U pip
          # Install repo deps but don't fail if someone pinned old openai
          python3 -m pip install -r requirements.txt || true
          # Ensure v1 SDK + essentials are present
          python3 -m pip install --upgrade "openai>=1.30.0" python-dotenv httpx pandas numpy pillow tqdm requests gdown

      - name: Install unzip
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip

      - name: Download dataset ZIP and unzip
        run: |
          set -euxo pipefail
          mkdir -p /tmp/dataset
          gdown "https://drive.google.com/uc?id=${{ github.event.inputs.drive_zip_file_id }}" -O /tmp/dataset/semi-aves.zip
          unzip -q /tmp/dataset/semi-aves.zip -d /tmp/dataset
          echo "After unzip:"
          find /tmp/dataset -maxdepth 2 -type d | sed 's/^/  /' | head -n 50

      - name: Sanity check dataset
        run: |
          set -euxo pipefail
          SEMI_AVES_DIR="$(find /tmp/dataset -maxdepth 2 -type d -name semi-aves | head -n1)"
          test -d "$SEMI_AVES_DIR/test"
          COUNT=$(find "$SEMI_AVES_DIR/test" -type f | wc -l)
          echo "Found $COUNT files under test/"
          [ "$COUNT" -ge 7000 ]

      - name: Expose dataset at ./datasets/semi-aves (as expected by wrapper)
        run: |
          mkdir -p datasets
          if [ -d /tmp/dataset/semi-aves ]; then
            ln -s /tmp/dataset/semi-aves datasets/semi-aves
          else
            CAND="$(find /tmp/dataset -maxdepth 2 -type d -name semi-aves | head -n1)"
            if [ -n "$CAND" ]; then
              ln -s "$CAND" datasets/semi-aves
            else
              echo "ERROR: Could not find 'semi-aves' folder in /tmp/dataset" >&2
              exit 1
            fi
          fi
          ls -la datasets

      - name: Provide .env for run_qwen.sh (NEBIUS_API_KEY)
        env:
          NEBIUS_API_KEY: ${{ secrets.NEBIUS_API_KEY }}
        run: |
          echo "NEBIUS_API_KEY=${NEBIUS_API_KEY}" > .env
          echo "NEBIUS_API_BASE=${{ github.event.inputs.api_base }}" >> .env
          echo ".env created at repo root"

      - name: Make scripts executable
        run: |
          chmod +x mllm-inference/semi-aves/scripts/run_nebius_72b.sh
          chmod +x mllm-inference/semi-aves/scripts/run_qwen.sh

      # ---------------- SHARDING ----------------
      - name: Split test.txt into shards
        run: |
          # Create zero-padded parts: test_part00.txt … test_part05.txt
          split -n l/${{ matrix.total }} -d -a 2 \
            data/semi-aves/test.txt data/semi-aves/test_part --additional-suffix=.txt
          ls -la data/semi-aves/test_part*.txt
          wc -l data/semi-aves/test_part*.txt

      - name: Use this shard as test.txt (zero-padded index)
        run: |
          SH=$(printf "%02d" ${{ matrix.shard }})
          cp "data/semi-aves/test_part${SH}.txt" data/semi-aves/test.txt
          echo "Preview of this shard:"
          sed -n '1,5p' data/semi-aves/test.txt
          echo "Shard line count:"; wc -l data/semi-aves/test.txt

      - name: Clear proxy env (guard)
        run: |
          unset HTTP_PROXY HTTPS_PROXY http_proxy https_proxy ALL_PROXY all_proxy || true

      - name: Run your wrapper (which calls run_qwen.sh)
        run: |
          mllm-inference/semi-aves/scripts/run_nebius_72b.sh

      - name: Collect & upload outputs (tag with shard id)
        run: |
          set -euxo pipefail
          mkdir -p outputs
          shopt -s nullglob
          for f in mllm-inference/semi-aves/mllm_output/*.{csv,txt,log}; do
            base="$(basename "$f")"
            ext="${base##*.}"
            stem="${base%.*}"
            cp "$f" "outputs/${stem}_shard$(printf "%02d" ${{ matrix.shard }}).${ext}"
          done
          find outputs -maxdepth 1 -type f -print || true

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: qwen-results-shard-$(printf "%02d" ${{ matrix.shard }})
          path: outputs/
          if-no-files-found: warn
